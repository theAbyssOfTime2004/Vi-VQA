{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Vi-VQA Training on Google Colab\n",
    "\n",
    "Notebook ƒë·ªÉ train Qwen3-VL tr√™n Google Colab v·ªõi GPU mi·ªÖn ph√≠.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab v·ªõi GPU (T4, L4, ho·∫∑c A100)\n",
    "- HuggingFace token\n",
    "- ~15GB disk space\n",
    "\n",
    "**Training time:**\n",
    "- T4: ~30-40 gi·ªù (slow, kh√¥ng recommend)\n",
    "- L4: ~15-20 gi·ªù\n",
    "- A100: ~8-12 gi·ªù (Colab Pro+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f'\\nPyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repo (replace with your actual repo URL)\n",
    "# Option 1: From GitHub\n",
    "# !git clone https://github.com/your-username/Vi-VQA.git\n",
    "# %cd Vi-VQA\n",
    "\n",
    "# Option 2: Upload files manually or use Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# If you have project in Drive:\n",
    "# %cd /content/drive/MyDrive/Vi-VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers from source (required for Qwen3-VL)\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q qwen-vl-utils accelerate peft bitsandbytes datasets pillow pyyaml huggingface_hub scipy\n",
    "\n",
    "# Install flash-attention (optional, speeds up training)\n",
    "# May take 5-10 minutes to compile\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "# Option 1: Use Colab secrets (recommended)\n",
    "# Go to: üîë icon on left sidebar ‚Üí Add secret: HF_TOKEN\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print('‚úì Logged in using Colab secret')\n",
    "except:\n",
    "    # Option 2: Enter token manually\n",
    "    print('HF_TOKEN not found in secrets. Please enter manually:')\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset\n",
    "\n",
    "### Create necessary directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/images', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "print('‚úì Directories created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print('Loading dataset from HuggingFace...')\n",
    "dataset = load_dataset('5CD-AI/Viet-ViTextVQA-gemini-VQA', split='train')\n",
    "print(f'‚úì Loaded {len(dataset)} samples')\n",
    "print(f'Columns: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process dataset to Qwen3-VL format\nprocessed_samples = []\nimage_folder = 'data/images'\n\nprint('Processing dataset...')\nfor idx in tqdm(range(len(dataset))):\n    item = dataset[idx]\n    image = item['image']\n    conversations = item.get('conversations', [])\n    \n    if not conversations:\n        continue\n    \n    # Save image\n    image_filename = f\"image_{item['id']}.jpg\"\n    image_path = os.path.join(image_folder, image_filename)\n    \n    if not os.path.exists(image_path):\n        try:\n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            image.save(image_path)\n        except:\n            continue\n    \n    # Process conversations\n    current_question = None\n    for turn in conversations:\n        role = turn.get('role', turn.get('from'))\n        content = turn.get('content', turn.get('value'))\n        \n        if role in ['user', 'human']:\n            current_question = content\n        elif role in ['assistant', 'gpt'] and current_question:\n            processed_samples.append({\n                'id': f\"{item['id']}_{len(processed_samples)}\",\n                'image': image_filename,\n                'conversations': [\n                    {'from': 'human', 'value': f'<image>\\n{current_question}'},\n                    {'from': 'gpt', 'value': content}\n                ]\n            })\n            current_question = None\n\nprint(f'‚úì Processed {len(processed_samples)} QA pairs')\n\n# Split dataset: 90% train, 10% validation\nimport random\nrandom.seed(42)  # For reproducibility\n\n# Shuffle samples\nshuffled_samples = processed_samples.copy()\nrandom.shuffle(shuffled_samples)\n\n# Calculate split index\nsplit_idx = int(len(shuffled_samples) * 0.9)\ntrain_samples = shuffled_samples[:split_idx]\nval_samples = shuffled_samples[split_idx:]\n\nprint(f'\\nüìä Dataset Split:')\nprint(f'  Train: {len(train_samples)} samples ({len(train_samples)/len(processed_samples)*100:.1f}%)')\nprint(f'  Val:   {len(val_samples)} samples ({len(val_samples)/len(processed_samples)*100:.1f}%)')\n\n# Save train set\nwith open('data/train.json', 'w', encoding='utf-8') as f:\n    json.dump(train_samples, f, ensure_ascii=False, indent=2)\nprint('‚úì Saved train set to data/train.json')\n\n# Save validation set\nwith open('data/val.json', 'w', encoding='utf-8') as f:\n    json.dump(val_samples, f, ensure_ascii=False, indent=2)\nprint('‚úì Saved validation set to data/val.json')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f'Dataset Statistics:')\nprint(f'  Total samples: {len(processed_samples)}')\nprint(f'  Train samples: {len(train_samples)}')\nprint(f'  Val samples: {len(val_samples)}')\nprint(f'  Images saved: {len(os.listdir(\"data/images\"))}')\nprint(f'\\nFirst training sample:')\nprint(json.dumps(train_samples[0], ensure_ascii=False, indent=2))\nprint(f'\\nFirst validation sample:')\nprint(json.dumps(val_samples[0], ensure_ascii=False, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Training\n",
    "\n",
    "### Clone Qwen-VL-Series-Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Qwen-VL-Series-Finetune'):\n",
    "    !git clone https://github.com/2U1/Qwen-VL-Series-Finetune.git\n",
    "    print('‚úì Cloned training repository')\n",
    "else:\n",
    "    print('‚úì Training repository already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nMODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"\nDATA_PATH = \"data/train.json\"\nVAL_DATA_PATH = \"data/val.json\"  # Validation set\nIMAGE_FOLDER = \"data/images\"\nOUTPUT_DIR = \"checkpoints/qwen3vl-vivqa\"\n\n# Hyperparameters\nNUM_EPOCHS = 2  # Reduced from 3 to avoid overfitting (large dataset)\nBATCH_SIZE = 1  # Small for Colab (adjust based on GPU)\nGRAD_ACCUM = 16  # Effective batch size = 1 * 16 = 16\nLEARNING_RATE = 2e-5\nVISION_LR = 2e-6\nMERGER_LR = 2e-5\n\n# LoRA config\nLORA_RANK = 128\nLORA_ALPHA = 256\nLORA_DROPOUT = 0.05\n\n# Image resolution\nIMAGE_MIN_PIXELS = 256 * 32 * 32  # 262144\nIMAGE_MAX_PIXELS = 1280 * 32 * 32  # 1310720\n\n# Evaluation config\nEVAL_STRATEGY = \"steps\"  # Evaluate every N steps\nEVAL_STEPS = 500  # Evaluate every 500 steps\nSAVE_STRATEGY = \"steps\"  # Save checkpoints based on steps\nSAVE_STEPS = 500  # Save every 500 steps\nSAVE_TOTAL_LIMIT = 3  # Keep only best 3 checkpoints\nLOAD_BEST_MODEL_AT_END = True  # Load best model when training ends\nMETRIC_FOR_BEST_MODEL = \"eval_loss\"  # Use validation loss to select best model\n\nprint('Training Configuration:')\nprint(f'  Model: {MODEL_ID}')\nprint(f'  Train data: {len(train_samples)} samples')\nprint(f'  Val data: {len(val_samples)} samples')\nprint(f'  Batch size: {BATCH_SIZE} √ó {GRAD_ACCUM} = {BATCH_SIZE * GRAD_ACCUM}')\nprint(f'  Epochs: {NUM_EPOCHS}')\nprint(f'  Eval every: {EVAL_STEPS} steps')\nprint(f'  Output: {OUTPUT_DIR}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Training\n",
    "\n",
    "‚ö†Ô∏è **Important:** Training s·∫Ω m·∫•t nhi·ªÅu gi·ªù. Colab c√≥ th·ªÉ disconnect n·∫øu idle qu√° l√¢u.\n",
    "\n",
    "**Tips ƒë·ªÉ tr√°nh disconnect:**\n",
    "- M·ªü F12 Console v√† ch·∫°y: `setInterval(() => { document.querySelector('colab-connect-button').click() }, 60000)`\n",
    "- D√πng Colab Pro ƒë·ªÉ tr√°nh timeout\n",
    "- Save checkpoints th∆∞·ªùng xuy√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%cd Qwen-VL-Series-Finetune\n\n# Build training command with validation\ntrain_cmd = f\"\"\"\npython train.py \\\n    --model_id {MODEL_ID} \\\n    --data_path ../{DATA_PATH} \\\n    --eval_data_path ../{VAL_DATA_PATH} \\\n    --image_folder ../{IMAGE_FOLDER} \\\n    --output_dir ../{OUTPUT_DIR} \\\n    --num_train_epochs {NUM_EPOCHS} \\\n    --per_device_train_batch_size {BATCH_SIZE} \\\n    --per_device_eval_batch_size {BATCH_SIZE} \\\n    --gradient_accumulation_steps {GRAD_ACCUM} \\\n    --learning_rate {LEARNING_RATE} \\\n    --vision_lr {VISION_LR} \\\n    --merger_lr {MERGER_LR} \\\n    --lora_rank {LORA_RANK} \\\n    --lora_alpha {LORA_ALPHA} \\\n    --lora_dropout {LORA_DROPOUT} \\\n    --num_lora_modules -1 \\\n    --image_min_pixels {IMAGE_MIN_PIXELS} \\\n    --image_max_pixels {IMAGE_MAX_PIXELS} \\\n    --freeze_vision_tower true \\\n    --freeze_llm false \\\n    --freeze_merger false \\\n    --optim adamw_bnb_8bit \\\n    --weight_decay 0.01 \\\n    --warmup_ratio 0.03 \\\n    --bf16 true \\\n    --gradient_checkpointing true \\\n    --max_grad_norm 1.0 \\\n    --dataloader_num_workers 2 \\\n    --eval_strategy {EVAL_STRATEGY} \\\n    --eval_steps {EVAL_STEPS} \\\n    --save_strategy {SAVE_STRATEGY} \\\n    --save_steps {SAVE_STEPS} \\\n    --save_total_limit {SAVE_TOTAL_LIMIT} \\\n    --load_best_model_at_end {str(LOAD_BEST_MODEL_AT_END).lower()} \\\n    --metric_for_best_model {METRIC_FOR_BEST_MODEL} \\\n    --greater_is_better false \\\n    --logging_steps 50 \\\n    --report_to tensorboard\n\"\"\"\n\nprint('Starting training with validation monitoring...')\nprint('='*80)\nprint('üìä Validation will run every 500 steps')\nprint('üíæ Best checkpoint will be saved based on validation loss')\nprint('üìà Monitor training progress in TensorBoard (next cell)')\nprint('='*80)\n!{train_cmd}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitor Training\n",
    "\n",
    "### Load TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../checkpoints/qwen3vl-vivqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Inference\n",
    "\n",
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%cd ..\n\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nimport torch\n\n# Load model checkpoint (adjust path to your best checkpoint)\nCHECKPOINT_PATH = \"checkpoints/qwen3vl-vivqa/checkpoint-1500\"  # Change this\n\nprint(f'Loading model from {CHECKPOINT_PATH}...')\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n    CHECKPOINT_PATH,\n    dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\nprocessor = AutoProcessor.from_pretrained(CHECKPOINT_PATH)\nprint('‚úì Model loaded!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ask_question(image_path, question, max_tokens=512):\n    \"\"\"Ask a question about an image using Method 1 (memory efficient)\"\"\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": image_path},\n                {\"type\": \"text\", \"text\": question}\n            ]\n        }\n    ]\n    \n    # Method 1: Direct tokenization (memory efficient)\n    inputs = processor.apply_chat_template(\n        messages,\n        tokenize=True,  # ‚úÖ Tokenize in one step\n        add_generation_prompt=True,\n        return_dict=True,\n        return_tensors=\"pt\"\n    )\n    inputs = inputs.to(model.device)\n    \n    with torch.inference_mode():\n        generated_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n    \n    generated_ids_trimmed = [\n        out_ids[len(in_ids):]\n        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    \n    answer = processor.batch_decode(\n        generated_ids_trimmed,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False\n    )[0]\n    \n    # Clean up memory\n    del inputs\n    del generated_ids\n    torch.cuda.empty_cache()\n    \n    return answer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi m·ªôt m·∫´u t·ª´ dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load m·ªôt sample\n",
    "test_sample = processed_samples[0]\n",
    "image_path = os.path.join('data/images', test_sample['image'])\n",
    "question = test_sample['conversations'][0]['value'].replace('<image>\\n', '')\n",
    "ground_truth = test_sample['conversations'][1]['value']\n",
    "\n",
    "# Display image\n",
    "img = Image.open(image_path)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image')\n",
    "plt.show()\n",
    "\n",
    "# Generate answer\n",
    "print(f'Question: {question}')\n",
    "print(f'Ground Truth: {ground_truth}')\n",
    "print('\\nGenerating answer...')\n",
    "\n",
    "prediction = ask_question(image_path, question)\n",
    "print(f'Prediction: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Multiple Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from difflib import SequenceMatcher\n\ndef similarity(a, b):\n    return SequenceMatcher(None, a, b).ratio()\n\n# Load validation set\nwith open('data/val.json', 'r', encoding='utf-8') as f:\n    val_data = json.load(f)\n\n# Evaluate on validation set\n# Option 1: Full validation set (takes longer)\n# num_eval = len(val_data)\n# Option 2: Subset for quick check (faster)\nnum_eval = min(100, len(val_data))  # Evaluate on first 100 samples\n\nexact_matches = 0\nsimilarities = []\n\nprint(f'Evaluating on {num_eval} validation samples...')\nprint('(This may take a while...)\\n')\n\nfor i in tqdm(range(num_eval)):\n    sample = val_data[i]\n    image_path = os.path.join('data/images', sample['image'])\n    question = sample['conversations'][0]['value'].replace('<image>\\n', '')\n    ground_truth = sample['conversations'][1]['value']\n    \n    try:\n        prediction = ask_question(image_path, question)\n        \n        if prediction.strip() == ground_truth.strip():\n            exact_matches += 1\n        \n        sim = similarity(prediction.lower(), ground_truth.lower())\n        similarities.append(sim)\n        \n        # Print first 3 examples\n        if i < 3:\n            print(f'\\n--- Example {i+1} ---')\n            print(f'Q: {question}')\n            print(f'GT: {ground_truth}')\n            print(f'Pred: {prediction}')\n            print(f'Similarity: {sim*100:.1f}%')\n    except Exception as e:\n        print(f'Error on sample {i}: {e}')\n        continue\n\n# Results\nprint(f'\\n{\"=\"*80}')\nprint('Validation Results')\nprint(f'{\"=\"*80}')\nprint(f'Samples evaluated: {num_eval}/{len(val_data)}')\nprint(f'Exact Match: {exact_matches}/{num_eval} ({exact_matches/num_eval*100:.2f}%)')\nprint(f'Avg Similarity: {sum(similarities)/len(similarities)*100:.2f}%')\nprint(f'{\"=\"*80}')\nprint('\\nüí° Tip: Change num_eval to len(val_data) to evaluate full validation set')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model to Drive\n",
    "\n",
    "‚ö†Ô∏è **Important:** L∆∞u model v√†o Google Drive ƒë·ªÉ kh√¥ng m·∫•t khi Colab disconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoint to Google Drive\n",
    "import shutil\n",
    "\n",
    "drive_output_dir = '/content/drive/MyDrive/Vi-VQA-Models'\n",
    "os.makedirs(drive_output_dir, exist_ok=True)\n",
    "\n",
    "# Copy best checkpoint\n",
    "print('Copying checkpoint to Google Drive...')\n",
    "shutil.copytree(\n",
    "    CHECKPOINT_PATH,\n",
    "    os.path.join(drive_output_dir, os.path.basename(CHECKPOINT_PATH)),\n",
    "    dirs_exist_ok=True\n",
    ")\n",
    "\n",
    "print(f'‚úì Checkpoint saved to {drive_output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Model (Optional)\n",
    "\n",
    "N·∫øu mu·ªën download v·ªÅ m√°y local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip checkpoint\n",
    "!zip -r qwen3vl-vivqa-checkpoint.zip {CHECKPOINT_PATH}\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('qwen3vl-vivqa-checkpoint.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Save model to Drive** (cell 8) ƒë·ªÉ kh√¥ng m·∫•t khi disconnect\n",
    "2. **Evaluate thoroughly** tr√™n validation set\n",
    "3. **Fine-tune hyperparameters** n·∫øu c·∫ßn\n",
    "4. **Deploy** model cho production\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- **OOM Error?** Gi·∫£m `BATCH_SIZE` xu·ªëng 1, tƒÉng `GRAD_ACCUM`\n",
    "- **Training qu√° ch·∫≠m?** Upgrade l√™n Colab Pro (A100)\n",
    "- **Model kh√¥ng h·ªçc?** Check learning rate, c√≥ th·ªÉ qu√° cao/th·∫•p\n",
    "- **Overfitting?** Gi·∫£m epochs ho·∫∑c th√™m data augmentation\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}