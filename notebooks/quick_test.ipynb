{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Quick Test - FIXED Version\n",
    "\n",
    "Dá»±a trÃªn code Ä‘Ã£ work tá»« qwen3-vl-8b-caption-gen.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (FIXED versions)\n",
    "!pip install -q transformers==4.57.0\n",
    "!pip install -q qwen-vl-utils accelerate peft bitsandbytes datasets pillow pyyaml huggingface_hub scipy\n",
    "\n",
    "# Optional: flash-attention (cÃ³ thá»ƒ skip)\n",
    "# !pip install -q flash-attn --no-build-isolation\n",
    "\n",
    "print('âœ… Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login HuggingFace\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Colab secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "except:\n",
    "    # Option 2: Manual\n",
    "    login()\n",
    "\n",
    "print('âœ… Logged in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (100 samples for test)\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "dataset = load_dataset('5CD-AI/Viet-ViTextVQA-gemini-VQA', split='train')\n",
    "print(f'âœ… Dataset loaded: {len(dataset)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 100 samples\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/images', exist_ok=True)\n",
    "\n",
    "TEST_SAMPLES = 100\n",
    "processed_samples = []\n",
    "\n",
    "for idx in tqdm(range(min(TEST_SAMPLES, len(dataset)))):\n",
    "    item = dataset[idx]\n",
    "    image = item['image']\n",
    "    conversations = item.get('conversations', [])\n",
    "    \n",
    "    if not conversations:\n",
    "        continue\n",
    "    \n",
    "    # Save image\n",
    "    image_filename = f\"image_{item['id']}.jpg\"\n",
    "    image_path = os.path.join('data/images', image_filename)\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image.save(image_path)\n",
    "    \n",
    "    # Process conversations\n",
    "    current_question = None\n",
    "    for turn in conversations:\n",
    "        role = turn.get('role', turn.get('from'))\n",
    "        content = turn.get('content', turn.get('value'))\n",
    "        \n",
    "        if role in ['user', 'human']:\n",
    "            current_question = content\n",
    "        elif role in ['assistant', 'gpt'] and current_question:\n",
    "            processed_samples.append({\n",
    "                'id': f\"{item['id']}_{len(processed_samples)}\",\n",
    "                'image': image_filename,\n",
    "                'conversations': [\n",
    "                    {'from': 'human', 'value': f'<image>\\\\n{current_question}'},\n",
    "                    {'from': 'gpt', 'value': content}\n",
    "                ]\n",
    "            })\n",
    "            current_question = None\n",
    "\n",
    "print(f'âœ… Processed {len(processed_samples)} QA pairs')\n",
    "\n",
    "# Save JSON\n",
    "with open('data/train_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(processed_samples, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('âœ… Saved to data/train_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL - FIXED VERSION\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor  # âœ… Qwen3 (not Qwen2)\n",
    "from qwen_vl_utils import process_vision_info  # âœ… Important import\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
    "\n",
    "print(f'Loading {MODEL_ID}...')\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16,  # âœ… dtype (not torch_dtype)\n",
    "    attn_implementation=\"flash_attention_2\",  # Comment out if no flash-attn\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "print('âœ… Model loaded successfully!')\n",
    "\n",
    "# Check memory\n",
    "allocated = torch.cuda.memory_allocated() / 1e9\n",
    "print(f'GPU Memory Allocated: {allocated:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST INFERENCE - FIXED VERSION\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test sample\n",
    "test_sample = processed_samples[0]\n",
    "image_path = os.path.join('data/images', test_sample['image'])\n",
    "question = test_sample['conversations'][0]['value'].replace('<image>\\\\n', '')\n",
    "ground_truth = test_sample['conversations'][1]['value']\n",
    "\n",
    "# Display image\n",
    "img = Image.open(image_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f'Question: {question}')\n",
    "print(f'Ground Truth: {ground_truth}')\n",
    "print('\\nGenerating...')\n",
    "\n",
    "# Prepare messages (Ä‘Ãºng theo format Ä‘Ã£ work)\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\", \"image\": image_path},\n",
    "        {\"type\": \"text\", \"text\": question}\n",
    "    ]\n",
    "}]\n",
    "\n",
    "# Apply chat template\n",
    "text = processor.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Process vision info\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# Generate\n",
    "with torch.inference_mode():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# Decode\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] \n",
    "    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "answer = processor.batch_decode(\n",
    "    generated_ids_trimmed, \n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(f'\\nBase Model Answer: {answer}')\n",
    "print('\\nâœ… Inference works!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… If Above Cell Works â†’ Training Will Work!\n",
    "\n",
    "Náº¿u cell trÃªn cháº¡y OK â†’ CÃ³ thá»ƒ proceed vá»›i full training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
