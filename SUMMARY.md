# ğŸ“Š Vi-VQA Project Summary - Qwen3-VL Implementation

## ğŸ¯ Project Goal

XÃ¢y dá»±ng há»‡ thá»‘ng **Visual Question Answering (VQA)** cho tiáº¿ng Viá»‡t sá»­ dá»¥ng **Qwen3-VL-8B-Instruct**, giáº£i quyáº¿t bÃ i toÃ¡n tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ná»™i dung hÃ¬nh áº£nh.

---

## ğŸ“ Project Structure

```
Vi-VQA/
â”œâ”€â”€ ğŸ“„ README.md                    # Main documentation
â”œâ”€â”€ ğŸ“„ PIPELINE.md                  # Complete training pipeline
â”œâ”€â”€ ğŸ“„ SUMMARY.md                   # This file
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ setup_vlm.sh                 # Environment setup script
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                 # Model & training configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/                     # Extracted images (created on run)
â”‚   â”œâ”€â”€ train.json                  # Training data (created on run)
â”‚   â””â”€â”€ test.json                   # Test data (created on run)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py                  # Legacy dataset (baseline)
â”‚   â”œâ”€â”€ dataset_vlm.py              # VLM dataset processor â­
â”‚   â”œâ”€â”€ inference_qwen3vl.py        # Inference & evaluation â­
â”‚   â”œâ”€â”€ inspect_data.py             # Data inspection tool
â”‚   â”œâ”€â”€ utils.py                    # Utility functions
â”‚   â””â”€â”€ vocab.py                    # Legacy vocab (not used)
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_qwen3vl.sh            # Training script â­
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb                   # Exploratory Data Analysis âœ…
â”‚   â”œâ”€â”€ test_qwen3vl.ipynb          # Model testing notebook
â”‚   â””â”€â”€ quick_stats.py              # Quick statistics script
â”‚
â”œâ”€â”€ checkpoints/                     # Model checkpoints (created on run)
â”œâ”€â”€ logs/                            # Training logs (created on run)
â””â”€â”€ Vi-VQA/                          # Virtual environment

â­ = Core implementation files
âœ… = Completed analysis
```

---

## ğŸ“Š Dataset Analysis Summary

### **Dataset:** Viet-ViTextVQA-gemini-VQA
- **Source:** [HuggingFace](https://huggingface.co/datasets/5CD-AI/Viet-ViTextVQA-gemini-VQA)
- **Images:** 9,594
- **QA Pairs:** 31,420
- **Average QA/image:** 3.27
- **Unique Answers:** 39,886

### **Key Statistics:**

| Metric | Value |
|--------|-------|
| Avg Question Length | 37.08 chars (~5-8 words) |
| Avg Answer Length | 48.97 chars (~7-10 words) |
| Avg Description Length | 557.75 chars |
| Image Size | 960Ã—1440 (typical) |

### **Critical Discovery from EDA:**

**âŒ Classification Approach is NOT viable:**

| Top-K Vocab | Coverage | Problem |
|-------------|----------|---------|
| 100         | 0.95%    | Too low |
| 500         | 3.25%    | Insufficient |
| 1,000       | 4.88%    | Still poor |
| 5,000       | 17.61%   | Unacceptable |

**Reason:**
- 39,886 unique answers vá»›i long-tail distribution
- Most answers appear only 1-2 times
- Generative nature of responses

**âœ… Solution:** Use generative VLM (Qwen3-VL)

---

## ğŸ—ï¸ Architecture Decision

### **Rejected Approaches:**

1. **âŒ Baseline Classification Model**
   ```
   ViT â†’ [CLS] token
                      âŸ¶ Concat âŸ¶ MLP âŸ¶ Softmax(1000 classes)
   PhoBERT â†’ [CLS]
   ```
   - **Problem:** Vocab coverage <5%
   - **Verdict:** Not suitable for this dataset

2. **âŒ Seq2Seq Model**
   ```
   ViT + PhoBERT â†’ Encoder âŸ¶ Decoder (GPT-style) â†’ Generate Answer
   ```
   - **Problem:** Needs training from scratch
   - **Verdict:** Too expensive, no pre-trained knowledge

### **âœ… Selected: Qwen3-VL with LoRA**

```
Qwen3-VL-8B-Instruct
    â”œâ”€â”€ Vision Encoder (ViT-based) [FROZEN]
    â”œâ”€â”€ Vision-Language Projector [TRAINABLE]
    â””â”€â”€ Language Model (8B params) [TRAINABLE with LoRA]
```

**Advantages:**
1. âœ… Pre-trained on 32 languages including Vietnamese
2. âœ… SOTA vision-language understanding
3. âœ… Efficient fine-tuning with LoRA (train on RTX 3090)
4. âœ… Generative â†’ handles infinite answer space
5. âœ… 256K context window
6. âœ… Community support & documentation

---

## ğŸ”§ Implementation Details

### **Dataset Processing (dataset_vlm.py)**

**Input:** HuggingFace dataset
```python
{
    'id': 0,
    'image': PIL.Image,
    'description': "Bá»©c áº£nh...",
    'conversations': [
        {'role': 'user', 'content': 'Question 1?'},
        {'role': 'assistant', 'content': 'Answer 1.'},
        {'role': 'user', 'content': 'Question 2?'},
        {'role': 'assistant', 'content': 'Answer 2.'}
    ]
}
```

**Process:**
1. Extract multi-turn conversations
2. Split into individual QA pairs
3. Save images to disk
4. Convert to Qwen-VL training format

**Output:** JSON for training
```json
{
    "id": "0_0",
    "image": "image_0.jpg",
    "conversations": [
        {"from": "human", "value": "<image>\nQuestion?"},
        {"from": "gpt", "value": "Answer."}
    ]
}
```

**Result:** 31,420 training samples

---

### **Model Configuration (config.yaml)**

```yaml
model:
  type: "vlm"
  vlm:
    model_id: "Qwen/Qwen3-VL-8B-Instruct"
    use_flash_attn: true
    torch_dtype: "bfloat16"

    lora:
      enabled: true
      rank: 128        # LoRA rank
      alpha: 256       # Scaling factor
      dropout: 0.05
      target_modules: "all"
      vision_lora: false  # Freeze vision encoder

training:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2e-5
  vision_lr: 2e-6
  freeze_vision_tower: true
  bf16: true
  gradient_checkpointing: true
```

**Key Decisions:**
- **LoRA only on LLM:** Vision encoder frozen to save memory
- **Small batch size + gradient accumulation:** Fit in 24GB GPU
- **BF16 precision:** Better stability than FP16
- **3 epochs:** Prevent overfitting on small dataset

---

### **Training Script (train_qwen3vl.sh)**

**Process:**
1. Clone Qwen-VL-Series-Finetune repo
2. Load base model + LoRA adapters
3. Train with config parameters
4. Save checkpoints every 500 steps
5. Log to TensorBoard

**GPU Requirements:**
- **Minimum:** RTX 3090 (24GB) with LoRA
- **Recommended:** A100 (40GB)
- **For QLoRA (4-bit):** RTX 3080 (10GB) works

**Training Time:**
- RTX 3090: ~20-30 hours
- A100: ~10-15 hours

---

### **Inference (inference_qwen3vl.py)**

**Modes:**

1. **Interactive Mode:**
   ```bash
   python src/inference_qwen3vl.py \
       --model_path ./checkpoints/qwen3vl-vivqa \
       --mode interactive
   ```
   - Ask questions about any image
   - Real-time responses

2. **Batch Evaluation:**
   ```bash
   python src/inference_qwen3vl.py \
       --model_path ./checkpoints/qwen3vl-vivqa \
       --mode eval \
       --test_data ./data/test.json \
       --output ./predictions.json
   ```
   - Evaluate on test set
   - Calculate metrics (accuracy, BLEU, ROUGE)

---

## ğŸ“ˆ Expected Performance

### **Before Fine-tuning (Base Model):**
- Exact Match: ~5-10%
- BLEU-4: ~15-20
- Issues:
  - Answers in English
  - Doesn't understand Vietnamese cultural context
  - Generic responses

### **After Fine-tuning (Expected):**
- Exact Match: ~40-60%
- BLEU-4: ~50-60
- Similarity: ~70-85%
- Improvements:
  - âœ… Vietnamese fluency
  - âœ… Domain knowledge (Vietnamese landmarks)
  - âœ… Natural, contextual answers

---

## ğŸ”‘ Key Features

### 1. **Efficient Training**
- LoRA reduces trainable params from 8B to ~100M
- Gradient checkpointing saves memory
- 8-bit optimizer (AdamW) reduces memory by 2x

### 2. **Flexible Configuration**
- YAML-based config for easy tuning
- Switch between LoRA/QLoRA/Full fine-tuning
- Adjustable image resolution

### 3. **Comprehensive Evaluation**
- Multiple metrics (Exact Match, BLEU, ROUGE, Similarity)
- Interactive testing mode
- Jupyter notebook for visualization

### 4. **Production-Ready**
- Clean code structure
- Detailed documentation
- Error handling
- Logging

---

## ğŸš€ Quick Start Guide

```bash
# 1. Setup
bash setup_vlm.sh
source Vi-VQA/bin/activate

# 2. Login
huggingface-cli login

# 3. Prepare data
python src/dataset_vlm.py

# 4. Train
bash scripts/train_qwen3vl.sh

# 5. Test
python src/inference_qwen3vl.py \
    --model_path ./checkpoints/qwen3vl-vivqa \
    --mode interactive
```

---

## ğŸ“Š EDA Highlights

**From `notebooks/eda.ipynb`:**

### 1. Conversation Distribution
- Peak: 2-3 QA pairs per image (>4,000 images)
- Long-tail: Some images with 10+ QA pairs
- Distribution: Skewed, not uniform

### 2. Question Types
- "ÄÃ¢y lÃ ...?" (What is this?)
- "á» Ä‘Ã¢u?" (Where?)
- "Thá»i gian...?" (When?)
- "CÃ³ gÃ¬ trong áº£nh?" (What's in the image?)

### 3. Answer Patterns
- Short factual answers (5-10 words)
- Some descriptive answers (20-50 words)
- Rare: Yes/No answers (<1%)

### 4. Top Answers
1. (14Ã—) "ÄÃ¢y lÃ  khu chá»£."
2. (8Ã—) "ÄÃ¢y lÃ  siÃªu thá»‹ VinMart."
3. (7Ã—) "ÄÃ¢y lÃ  há»‡ thá»‘ng siÃªu thá»‹ VinMart."
4. (6Ã—) "Bá»©c áº£nh nÃ y chá»¥p á»Ÿ chá»£."
5. (6Ã—) "ÄÃ¢y lÃ  trÆ°á»ng Äáº¡i há»c."

**Observation:** Even top answers have low frequency â†’ Need generative model

---

## ğŸ“ Lessons Learned

1. **Always do EDA first!**
   - Saved weeks of work on wrong approach (classification)
   - Discovered dataset characteristics early

2. **Choose architecture based on data**
   - Long-tail answer distribution â†’ Generative model
   - Not every problem needs custom architecture

3. **Pre-trained models are powerful**
   - Qwen3-VL already knows Vietnamese
   - Fine-tuning >> Training from scratch

4. **LoRA is game-changer**
   - Train 8B model on consumer GPU
   - 100Ã— fewer params to train

5. **Documentation matters**
   - Clear README helps future you
   - Pipeline doc ensures reproducibility

---

## ğŸ”® Future Improvements

### **Short-term:**
1. âœ… Data augmentation (paraphrasing, back-translation)
2. âœ… Hyperparameter tuning (learning rate, batch size)
3. âœ… Multi-GPU training with DeepSpeed
4. âœ… Better evaluation metrics (CIDEr, METEOR)

### **Long-term:**
1. ğŸš€ Deploy as API endpoint
2. ğŸš€ Create web interface
3. ğŸš€ Support video QA
4. ğŸš€ Multi-turn conversation
5. ğŸš€ Ensemble multiple models

---

## ğŸ“š References

### **Papers:**
- Qwen3-VL: [arXiv](https://arxiv.org/abs/2412.xxxxx)
- LoRA: [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)
- ViTextVQA: [arXiv:2404.10652](https://arxiv.org/abs/2404.10652)

### **Code:**
- Qwen3-VL: [HuggingFace](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)
- Training Repo: [GitHub](https://github.com/2U1/Qwen-VL-Series-Finetune)
- Dataset: [HuggingFace](https://huggingface.co/datasets/5CD-AI/Viet-ViTextVQA-gemini-VQA)

---

## ğŸ† Project Achievements

âœ… **Completed:**
- [x] Dataset analysis (EDA)
- [x] Architecture selection
- [x] Dataset preprocessing for VLM
- [x] Training pipeline
- [x] Inference script
- [x] Evaluation metrics
- [x] Documentation

â³ **Pending:**
- [ ] Model training (waiting for GPU)
- [ ] Hyperparameter tuning
- [ ] Deployment

---

## ğŸ’¡ Key Takeaways

1. **EDA is critical** â†’ Discovered classification won't work
2. **Use pre-trained models** â†’ Qwen3-VL > custom architecture
3. **LoRA is efficient** â†’ 8B model on 24GB GPU
4. **Documentation matters** â†’ Future-proof the project
5. **Generative > Classification** â†’ For long-tail distributions

---

**Project Status:** âœ… Ready for Training

**Next Step:** Run `bash scripts/train_qwen3vl.sh` and train the model!

---

*Last Updated: 2025-11-24*
