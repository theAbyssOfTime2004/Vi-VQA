# Vi-VQA: Vietnamese Visual Question Answering with Qwen3-VL

Dá»± Ã¡n Visual Question Answering (VQA) cho tiáº¿ng Viá»‡t sá»­ dá»¥ng **Qwen3-VL-8B-Instruct** - má»™t trong nhá»¯ng Vision Language Model máº¡nh nháº¥t hiá»‡n táº¡i.

## ğŸ“Š Dataset

**Viet-ViTextVQA-gemini-VQA** ([HuggingFace](https://huggingface.co/datasets/5CD-AI/Viet-ViTextVQA-gemini-VQA))
- **9,594 images** tá»« ViTextVQA dataset
- **31,420 QA pairs** Ä‘Æ°á»£c sinh bá»Ÿi Google Gemini 1.5 Flash
- Domain: Di tÃ­ch lá»‹ch sá»­ Viá»‡t Nam, landmarks, sáº£n pháº©m, v.v.
- Multi-turn conversations vá»›i cÃ¢u tráº£ lá»i generative

### Dataset Statistics (tá»« EDA):
```
- Trung bÃ¬nh 3.27 QA pairs/image
- Äá»™ dÃ i cÃ¢u há»i: ~37 kÃ½ tá»±
- Äá»™ dÃ i cÃ¢u tráº£ lá»i: ~49 kÃ½ tá»±
- 39,886 unique answers â†’ Generative task, khÃ´ng pháº£i classification
```

## ğŸ—ï¸ Architecture

**Model:** Qwen3-VL-8B-Instruct
- **Vision Encoder:** ViT-based visual encoder
- **Language Model:** 8B parameter Qwen3 LLM
- **Multimodal Fusion:** Vision-language projector
- **Context Length:** 256K native, expandable to 1M tokens
- **OCR Support:** 32 languages including Vietnamese

**Fine-tuning Strategy:**
- LoRA (Low-Rank Adaptation) vá»›i rank=128
- Freeze vision encoder, chá»‰ tune LLM + projector
- Batch size: 2 Ã— 8 gradient accumulation = 16 effective
- Learning rate: 2e-5 (LLM), 2e-6 (vision)

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone repository
git clone <your-repo-url>
cd Vi-VQA

# Run setup script
bash setup_vlm.sh

# Or manual installation:
python3 -m venv Vi-VQA
source Vi-VQA/bin/activate
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

### 2. Login to HuggingFace

```bash
huggingface-cli login
# Enter your token: hf_...
```

Hoáº·c trong Python:
```python
from huggingface_hub import login
login(token="hf_...")
```

### 3. Prepare Dataset

```bash
# Convert dataset to Qwen3-VL format
python src/dataset_vlm.py
```

Káº¿t quáº£:
- `data/train.json`: Training data in Qwen-VL format
- `data/images/`: Extracted images

### 4. Train Model

```bash
# Start training with LoRA
bash scripts/train_qwen3vl.sh
```

Training sáº½:
- Clone Qwen-VL-Series-Finetune repository
- Train vá»›i LoRA adapters
- Save checkpoints má»—i 500 steps
- Log to TensorBoard

**Xem training progress:**
```bash
tensorboard --logdir ./checkpoints/qwen3vl-vivqa/runs
```

### 5. Inference

**Interactive mode:**
```bash
python src/inference_qwen3vl.py \
    --model_path ./checkpoints/qwen3vl-vivqa \
    --mode interactive
```

**Evaluation mode:**
```bash
python src/inference_qwen3vl.py \
    --model_path ./checkpoints/qwen3vl-vivqa \
    --mode eval \
    --test_data ./data/test.json \
    --output ./predictions.json
```

## ğŸ“ Project Structure

```
Vi-VQA/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Configuration file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/                  # Extracted images
â”‚   â”œâ”€â”€ train.json              # Training data (Qwen-VL format)
â”‚   â””â”€â”€ test.json               # Test data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset_vlm.py          # Dataset processor for VLM
â”‚   â”œâ”€â”€ inference_qwen3vl.py    # Inference script
â”‚   â”œâ”€â”€ utils.py                # Utilities
â”‚   â””â”€â”€ vocab.py                # (Legacy, not used for VLM)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_qwen3vl.sh        # Training script
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda.ipynb               # Exploratory Data Analysis
â”œâ”€â”€ checkpoints/                 # Model checkpoints
â”œâ”€â”€ logs/                        # Training logs
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup_vlm.sh               # Environment setup script
â””â”€â”€ README.md                   # This file
```

## âš™ï¸ Configuration

Edit `config/config.yaml` Ä‘á»ƒ thay Ä‘á»•i hyperparameters:

```yaml
model:
  vlm:
    model_id: "Qwen/Qwen3-VL-8B-Instruct"
    lora:
      enabled: true
      rank: 128
      alpha: 256
      dropout: 0.05

training:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2e-5
  freeze_vision_tower: true
```

## ğŸ“Š Evaluation Metrics

- **Exact Match Accuracy:** % cÃ¢u tráº£ lá»i giá»‘ng há»‡t ground truth
- **BLEU Score:** Äá»™ tÆ°Æ¡ng Ä‘á»“ng n-gram
- **ROUGE Score:** Recall-oriented overlap
- **CIDEr:** Consensus-based metric

## ğŸ”¬ EDA Insights

Tá»« `notebooks/eda.ipynb`:

1. **Dataset khÃ´ng phÃ¹ há»£p cho Classification:**
   - Top 1000 answers chá»‰ cover 4.88% dataset
   - Top 5000 answers chá»‰ cover 17.61%
   - â†’ Cáº§n generative model

2. **PhÃ¢n bá»‘ Conversations:**
   - KhÃ´ng balanced: pháº§n lá»›n images cÃ³ 2-3 QA pairs
   - Long-tail: má»™t sá»‘ Ã­t cÃ³ >10 QA pairs

3. **Text Statistics:**
   - CÃ¢u há»i ngáº¯n (~5-8 tá»«)
   - CÃ¢u tráº£ lá»i Ä‘a dáº¡ng (1-100+ tá»«)
   - max_length=2048 lÃ  Ä‘á»§

## ğŸ› ï¸ Advanced Usage

### Train with QLoRA (4-bit)

Edit `config.yaml`:
```yaml
model:
  vlm:
    qlora:
      enabled: true
```

### Multi-GPU Training

```bash
# Use DeepSpeed
deepspeed --num_gpus=4 \
    Qwen-VL-Series-Finetune/train.py \
    --deepspeed ds_config_zero2.json \
    ...
```

### Merge LoRA Weights

```bash
cd Qwen-VL-Series-Finetune
bash scripts/merge_lora.sh
```

## ğŸ“š References

- **Qwen3-VL:** [HuggingFace](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)
- **Dataset:** [Viet-ViTextVQA-gemini-VQA](https://huggingface.co/datasets/5CD-AI/Viet-ViTextVQA-gemini-VQA)
- **Fine-tuning Repo:** [Qwen-VL-Series-Finetune](https://github.com/2U1/Qwen-VL-Series-Finetune)
- **ViTextVQA Paper:** [arXiv:2404.10652](https://arxiv.org/abs/2404.10652)

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repo
2. Create a feature branch
3. Submit a pull request

## ğŸ“„ License

MIT License (hoáº·c license cá»§a báº¡n)

## âš ï¸ Notes

- Qwen3-VL requires transformers>=4.57.0 (install from source)
- Flash Attention 2 highly recommended cho tá»‘c Ä‘á»™
- Dataset lÃ  gated, cáº§n request access trÃªn HuggingFace
- Training vá»›i full dataset (~30k samples) máº¥t ~10-15 giá» trÃªn A100

## ğŸ†˜ Troubleshooting

**OOM Error:**
```bash
# Giáº£m batch size hoáº·c enable QLoRA
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
```

**Flash Attention Error:**
```bash
# Disable flash attention
--disable_flash_attn2 true
```

**Dataset Loading Error:**
```bash
# Check HuggingFace authentication
huggingface-cli whoami
```
